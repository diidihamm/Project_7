{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69829eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================\n",
    "# ULTIMATE CLAUDE — NanoDiidi with Modern Transformer Architecture\n",
    "#========================================================\n",
    "#\n",
    "# This is NanoDiidi rebuilt from the ground up with every\n",
    "# architectural refinement used in frontier models like Claude:\n",
    "#\n",
    "#   1.  RoPE        — Rotary Position Embeddings (replaces learned pos emb)\n",
    "#   2.  RMSNorm     — Root Mean Square Normalization (replaces LayerNorm)\n",
    "#   3.  SwiGLU      — Gated feed-forward with SiLU activation\n",
    "#   4.  GQA         — Grouped Query Attention (shared K/V heads)\n",
    "#   5.  QK-Norm     — RMSNorm on Q and K before attention\n",
    "#   6.  MoE         — Mixture of Experts with top-k routing\n",
    "#   7.  Sliding Window Attention — bounded local attention\n",
    "#   8.  KV-Cache    — efficient autoregressive generation\n",
    "#   9.  Residual Scaling — 1/sqrt(n_layers) for training stability\n",
    "#  10.  Vision Encoder — ViT-style multi-modal support (optional)\n",
    "#  11.  Reward Model — scalar scoring head for RLHF\n",
    "#  12.  Speculative Decoding — draft-then-verify for fast inference\n",
    "#\n",
    "#========================================================\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 1. IMPORTS\n",
    "#========================================================\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "from typing import Optional, Callable, Tuple, Dict, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 2. CONFIGURATION\n",
    "#========================================================\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    # --- Model architecture ---\n",
    "    vocab_size: int = 100277       # tiktoken cl100k_base\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6                # query heads\n",
    "    n_kv_head: int = 2             # key/value heads (GQA: n_head must be divisible by n_kv_head)\n",
    "    n_embd: int = 384\n",
    "    block_size: int = 256          # max sequence length\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # MoE\n",
    "    n_expert: int = 4              # number of experts (1 = dense, no MoE)\n",
    "    n_expert_active: int = 2       # top-k experts routed per token\n",
    "    moe_every_n: int = 2           # apply MoE every N layers; others use dense SwiGLU\n",
    "\n",
    "    # Sliding window (0 = full attention in all layers)\n",
    "    sliding_window: int = 128\n",
    "\n",
    "    # Vision encoder (disabled by default for text-only training)\n",
    "    use_vision: bool = False\n",
    "    image_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    vision_n_layer: int = 4\n",
    "    vision_n_head: int = 4\n",
    "    vision_n_embd: int = 384\n",
    "\n",
    "    # --- Training ---\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 6e-4\n",
    "    warmup_iters: int = 500\n",
    "    max_iters: int = 5000\n",
    "    eval_interval: int = 500\n",
    "\n",
    "    # --- Derived properties ---\n",
    "    @property\n",
    "    def head_dim(self) -> int:\n",
    "        return self.n_embd // self.n_head\n",
    "\n",
    "    @property\n",
    "    def n_kv_groups(self) -> int:\n",
    "        return self.n_head // self.n_kv_head\n",
    "\n",
    "    @property\n",
    "    def n_patches(self) -> int:\n",
    "        return (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "\n",
    "# Instantiate default config\n",
    "config = ModelConfig()\n",
    "\n",
    "# --- Training settings ---\n",
    "TRAIN_NEW_MODEL = False\n",
    "TOKENIZATION_MODE = 'bpe'\n",
    "GITHUB_TOKEN_FILE = 'github_token_secret.txt'\n",
    "LOCAL_FILE = 'training_data_tinystories_ver_4.txt'\n",
    "TRAINING_DATA_URL = \"https://raw.githubusercontent.com/diidihamm/Project_7/main/training_data_tinystories_ver_3.txt\"\n",
    "MODEL_FILENAME = 'tiny_qa_model_2.pth'\n",
    "TRAINED_MODEL_URL = \"https://github.com/diidihamm/Project_7/releases/download/model-20260208_014754/tiny_qa_model_2.pth\"\n",
    "\n",
    "# --- RLHF settings ---\n",
    "RUN_RLHF = False          # Set True to run RLHF after base training\n",
    "RLHF_EPOCHS = 2           # PPO epochs\n",
    "RLHF_BATCH_SIZE = 4       # Prompts per PPO batch\n",
    "RLHF_LR = 1e-5            # Much smaller LR for RLHF (fine-tuning)\n",
    "RLHF_KL_COEFF = 0.1       # KL penalty — prevents policy from drifting too far from reference\n",
    "RLHF_CLIP_RANGE = 0.2     # PPO clipping range\n",
    "RLHF_PROMPTS = [\n",
    "    \"Tell me a short story about a cat.\",\n",
    "    \"Write a story about a little girl who finds a magic flower.\",\n",
    "    \"Tell me about a boy who learns to be brave.\",\n",
    "    \"Write a short story about friendship.\",\n",
    "    \"Tell me a bedtime story about the moon.\",\n",
    "    \"Write a story about a dog who goes on an adventure.\",\n",
    "    \"Tell me about a princess who saves a dragon.\",\n",
    "    \"Write a short story about sharing.\",\n",
    "]\n",
    "\n",
    "# --- Device ---\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 3. CORE COMPONENTS\n",
    "#========================================================\n",
    "\n",
    "# ---- 3a. RMSNorm ----\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Replaces LayerNorm. Simpler, faster, no mean-centering.\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        return self.weight * (x / rms)\n",
    "\n",
    "\n",
    "# ---- 3b. Rotary Position Embeddings (RoPE) ----\n",
    "class RotaryPositionEmbedding(nn.Module):\n",
    "    \"\"\"Replaces learned position embeddings. Encodes relative position\n",
    "    via rotation of Q/K vectors. Enables length extrapolation.\"\"\"\n",
    "    def __init__(self, dim: int, max_seq_len: int = 8192, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        self._build_cache(max_seq_len)\n",
    "\n",
    "    def _build_cache(self, seq_len: int):\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        self.register_buffer('cos_cache', freqs.cos(), persistent=False)\n",
    "        self.register_buffer('sin_cache', freqs.sin(), persistent=False)\n",
    "\n",
    "    def forward(self, seq_len: int, offset: int = 0) -> Tuple[Tensor, Tensor]:\n",
    "        end = offset + seq_len\n",
    "        if end > self.cos_cache.shape[0]:\n",
    "            self._build_cache(end)\n",
    "        return self.cos_cache[offset:end], self.sin_cache[offset:end]\n",
    "\n",
    "\n",
    "def apply_rope(x: Tensor, cos: Tensor, sin: Tensor) -> Tensor:\n",
    "    \"\"\"Rotate Q or K by position-dependent angles.\n",
    "    x: [B, n_heads, T, head_dim]  cos/sin: [T, head_dim//2]\"\"\"\n",
    "    d = x.shape[-1] // 2\n",
    "    x1, x2 = x[..., :d], x[..., d:]\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)  # [1, 1, T, d]\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "\n",
    "\n",
    "# ---- 3c. SwiGLU Feed-Forward ----\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"Gated FFN with SiLU. Three matrices instead of two.\n",
    "    gate path decides WHICH information passes; value path provides WHAT.\"\"\"\n",
    "    def __init__(self, dim: int, hidden_dim: Optional[int] = None, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = int(2 * dim * 4 / 3)\n",
    "            hidden_dim = 64 * ((hidden_dim + 63) // 64)\n",
    "        self.w_gate = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w_up   = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w_down = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.dropout(self.w_down(F.silu(self.w_gate(x)) * self.w_up(x)))\n",
    "\n",
    "\n",
    "# ---- 3d. Mixture of Experts ----\n",
    "class MoELayer(nn.Module):\n",
    "    \"\"\"N expert FFNs, router picks top-k per token. Each token only\n",
    "    activates k experts, so compute is k/N of full cost.\"\"\"\n",
    "    def __init__(self, dim: int, n_experts: int, n_active: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.n_experts = n_experts\n",
    "        self.n_active = n_active\n",
    "        self.router = nn.Linear(dim, n_experts, bias=False)\n",
    "        self.experts = nn.ModuleList([SwiGLU(dim, dropout=dropout) for _ in range(n_experts)])\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        B, T, D = x.shape\n",
    "        x_flat = x.view(-1, D)\n",
    "\n",
    "        # Router selects experts\n",
    "        router_logits = self.router(x_flat)\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        top_k_probs, top_k_idx = torch.topk(router_probs, self.n_active, dim=-1)\n",
    "        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # Dispatch tokens to experts\n",
    "        output = torch.zeros_like(x_flat)\n",
    "        for k in range(self.n_active):\n",
    "            for e in range(self.n_experts):\n",
    "                mask = (top_k_idx[:, k] == e)\n",
    "                if mask.any():\n",
    "                    expert_out = self.experts[e](x_flat[mask])\n",
    "                    output[mask] += top_k_probs[mask, k].unsqueeze(-1) * expert_out\n",
    "\n",
    "        # Load-balancing auxiliary loss\n",
    "        tokens_per_expert = torch.zeros(self.n_experts, device=x.device)\n",
    "        for k in range(self.n_active):\n",
    "            for e in range(self.n_experts):\n",
    "                tokens_per_expert[e] += (top_k_idx[:, k] == e).float().sum()\n",
    "        tokens_per_expert = tokens_per_expert / (B * T * self.n_active)\n",
    "        prob_per_expert = router_probs.mean(dim=0)\n",
    "        aux_loss = self.n_experts * (tokens_per_expert * prob_per_expert).sum()\n",
    "\n",
    "        return output.view(B, T, D), aux_loss\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 4. GROUPED QUERY ATTENTION\n",
    "#========================================================\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"GQA with RoPE, QK-Norm, optional sliding window, and KV-cache.\n",
    "    Multiple Q heads share fewer K/V heads → smaller KV-cache at inference.\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.n_kv_head = config.n_kv_head\n",
    "        self.n_kv_groups = config.n_kv_groups\n",
    "        self.head_dim = config.head_dim\n",
    "        self.sliding_window = config.sliding_window\n",
    "\n",
    "        # Projections: Q has n_head heads, K/V have n_kv_head heads\n",
    "        self.q_proj = nn.Linear(config.n_embd, config.n_head * config.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.n_embd, config.n_kv_head * config.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.n_embd, config.n_kv_head * config.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.n_head * config.head_dim, config.n_embd, bias=False)\n",
    "\n",
    "        # QK-Norm: prevents attention logits from exploding in deep networks\n",
    "        self.q_norm = RMSNorm(config.head_dim)\n",
    "        self.k_norm = RMSNorm(config.head_dim)\n",
    "\n",
    "        # RoPE (shared across all heads, operates on head_dim)\n",
    "        self.rope = RotaryPositionEmbedding(config.head_dim, config.block_size * 4)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: Tensor, kv_cache: Optional[Tuple[Tensor, Tensor]] = None\n",
    "                ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        # Project to Q (n_head), K (n_kv_head), V (n_kv_head)\n",
    "        q = self.q_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.n_kv_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.n_kv_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # QK-Norm\n",
    "        q = self.q_norm(q)\n",
    "        k = self.k_norm(k)\n",
    "\n",
    "        # RoPE: position-aware rotation\n",
    "        offset = 0 if kv_cache is None else kv_cache[0].shape[2]\n",
    "        cos, sin = self.rope(T, offset=offset)\n",
    "        q = apply_rope(q, cos, sin)\n",
    "        k = apply_rope(k, cos, sin)\n",
    "\n",
    "        # KV-cache: append new K/V to cached K/V from previous tokens\n",
    "        if kv_cache is not None:\n",
    "            k = torch.cat([kv_cache[0], k], dim=2)\n",
    "            v = torch.cat([kv_cache[1], v], dim=2)\n",
    "        new_kv_cache = (k, v)\n",
    "\n",
    "        # Expand K/V heads to match Q heads (GQA repeat)\n",
    "        k_expanded = k.repeat_interleave(self.n_kv_groups, dim=1)\n",
    "        v_expanded = v.repeat_interleave(self.n_kv_groups, dim=1)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        full_len = k_expanded.shape[2]\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn = (q @ k_expanded.transpose(-2, -1)) * scale\n",
    "\n",
    "        # Causal mask: each query attends only to past positions\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(T, full_len, device=x.device, dtype=torch.bool),\n",
    "            diagonal=full_len - T + 1\n",
    "        )\n",
    "\n",
    "        # Sliding window mask: restrict attention to recent positions\n",
    "        if self.sliding_window > 0 and full_len > self.sliding_window:\n",
    "            positions = torch.arange(full_len, device=x.device)\n",
    "            query_positions = torch.arange(full_len - T, full_len, device=x.device)\n",
    "            window_mask = (query_positions.unsqueeze(1) - positions.unsqueeze(0)) >= self.sliding_window\n",
    "            causal_mask = causal_mask | window_mask\n",
    "\n",
    "        attn = attn.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        out = (attn @ v_expanded).transpose(1, 2).contiguous().view(B, T, D)\n",
    "        return self.resid_dropout(self.o_proj(out)), new_kv_cache\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 5. TRANSFORMER BLOCK\n",
    "#========================================================\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Pre-norm block: RMSNorm → GQA → residual, RMSNorm → FFN/MoE → residual.\n",
    "    Residual outputs are scaled by 1/sqrt(n_layers) for stability.\"\"\"\n",
    "    def __init__(self, layer_idx: int, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(config.n_embd)\n",
    "        self.ffn_norm  = RMSNorm(config.n_embd)\n",
    "        self.attn = GroupedQueryAttention(config)\n",
    "\n",
    "        # MoE or dense SwiGLU\n",
    "        self.use_moe = (config.n_expert > 1) and (layer_idx % config.moe_every_n == 0)\n",
    "        if self.use_moe:\n",
    "            self.ffn = MoELayer(config.n_embd, config.n_expert, config.n_expert_active, config.dropout)\n",
    "        else:\n",
    "            self.ffn = SwiGLU(config.n_embd, dropout=config.dropout)\n",
    "\n",
    "        self.residual_scale = 1.0 / math.sqrt(config.n_layer)\n",
    "\n",
    "    def forward(self, x: Tensor, kv_cache=None) -> Tuple[Tensor, Tuple, float]:\n",
    "        # Attention\n",
    "        attn_out, new_kv_cache = self.attn(self.attn_norm(x), kv_cache)\n",
    "        x = x + self.residual_scale * attn_out\n",
    "\n",
    "        # FFN (MoE or dense)\n",
    "        aux_loss = 0.0\n",
    "        if self.use_moe:\n",
    "            ffn_out, aux_loss = self.ffn(self.ffn_norm(x))\n",
    "        else:\n",
    "            ffn_out = self.ffn(self.ffn_norm(x))\n",
    "        x = x + self.residual_scale * ffn_out\n",
    "\n",
    "        return x, new_kv_cache, aux_loss\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 6. VISION ENCODER (optional, for multi-modal)\n",
    "#========================================================\n",
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"ViT-style encoder: image → patches → embeddings → transformer → project.\n",
    "    Output is a sequence of visual tokens in the language model's embedding space.\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        n_patches = config.n_patches\n",
    "        patch_dim = config.patch_size * config.patch_size * 3\n",
    "\n",
    "        self.patch_embed = nn.Linear(patch_dim, config.vision_n_embd)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, n_patches, config.vision_n_embd) * 0.02)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'norm1': RMSNorm(config.vision_n_embd),\n",
    "                'attn': nn.MultiheadAttention(config.vision_n_embd, config.vision_n_head, batch_first=True),\n",
    "                'norm2': RMSNorm(config.vision_n_embd),\n",
    "                'ffn': SwiGLU(config.vision_n_embd),\n",
    "            })\n",
    "            for _ in range(config.vision_n_layer)\n",
    "        ])\n",
    "        self.norm_out = RMSNorm(config.vision_n_embd)\n",
    "        self.proj = nn.Linear(config.vision_n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, images: Tensor) -> Tensor:\n",
    "        B, C, H, W = images.shape\n",
    "        p = self.config.patch_size\n",
    "        patches = images.unfold(2, p, p).unfold(3, p, p)\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        patches = patches.view(B, -1, C * p * p)\n",
    "\n",
    "        x = self.patch_embed(patches) + self.pos_embed\n",
    "        for layer in self.layers:\n",
    "            normed = layer['norm1'](x)\n",
    "            x = x + layer['attn'](normed, normed, normed)[0]\n",
    "            x = x + layer['ffn'](layer['norm2'](x))\n",
    "        return self.proj(self.norm_out(x))\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 7. MAIN MODEL: UltimateClaude\n",
    "#========================================================\n",
    "class UltimateClaude(nn.Module):\n",
    "    \"\"\"The complete model. Same skeleton as NanoDiidi — decoder-only\n",
    "    transformer with causal attention — but every component is upgraded.\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Token embedding (NO learned position embedding — RoPE handles it)\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.emb_scale = math.sqrt(config.n_embd)\n",
    "        self.emb_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # Transformer stack\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(i, config) for i in range(config.n_layer)\n",
    "        ])\n",
    "        self.norm_out = RMSNorm(config.n_embd)\n",
    "\n",
    "        # Language model head (weight-tied with token embedding)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "\n",
    "        # Vision encoder (only if enabled)\n",
    "        self.vision_encoder = VisionEncoder(config) if config.use_vision else None\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def get_num_params(self, non_embedding: bool = True) -> int:\n",
    "        n = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n -= self.tok_emb.weight.numel()\n",
    "        return n\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: Optional[Tensor] = None,\n",
    "                images: Optional[Tensor] = None,\n",
    "                kv_caches: Optional[List] = None\n",
    "                ) -> Tuple[Tensor, Optional[Tensor], List]:\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Token embeddings with scaling\n",
    "        x = self.tok_emb(idx) * self.emb_scale\n",
    "\n",
    "        # Prepend vision tokens if images provided\n",
    "        if images is not None and self.vision_encoder is not None:\n",
    "            vis_tokens = self.vision_encoder(images)\n",
    "            x = torch.cat([vis_tokens, x], dim=1)\n",
    "            T = x.shape[1]\n",
    "\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        # Transformer layers\n",
    "        new_kv_caches = []\n",
    "        total_aux_loss = 0.0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            kv_cache = kv_caches[i] if kv_caches is not None else None\n",
    "            x, new_kv, aux_loss = layer(x, kv_cache)\n",
    "            new_kv_caches.append(new_kv)\n",
    "            total_aux_loss += aux_loss\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Compute loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            if images is not None and self.vision_encoder is not None:\n",
    "                logits = logits[:, self.config.n_patches:, :]\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            loss = loss + 0.01 * total_aux_loss  # MoE load-balancing\n",
    "\n",
    "        return logits, loss, new_kv_caches\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int,\n",
    "                 temperature: float = 0.8, top_k: int = 40, top_p: float = 0.9,\n",
    "                 repetition_penalty: float = 1.2,\n",
    "                 stop_strings: Optional[List[str]] = None,\n",
    "                 decode_fn: Optional[Callable] = None,\n",
    "                 callback: Optional[Callable] = None) -> Tensor:\n",
    "        \"\"\"Autoregressive generation with KV-cache for efficiency.\"\"\"\n",
    "        self.eval()\n",
    "        kv_caches = None\n",
    "        all_generated = []\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            if kv_caches is not None:\n",
    "                idx_input = idx[:, -1:]\n",
    "            else:\n",
    "                idx_input = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "\n",
    "            logits, _, kv_caches = self(idx_input, kv_caches=kv_caches)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Repetition penalty\n",
    "            if repetition_penalty != 1.0:\n",
    "                for token_id in set(idx[0].tolist()):\n",
    "                    if logits[0, token_id] > 0:\n",
    "                        logits[0, token_id] /= repetition_penalty\n",
    "                    else:\n",
    "                        logits[0, token_id] *= repetition_penalty\n",
    "\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Top-k filtering\n",
    "            if top_k > 0:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "\n",
    "            # Top-p (nucleus) filtering\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n",
    "                cumulative = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                mask = (cumulative - F.softmax(sorted_logits, dim=-1)) > top_p\n",
    "                sorted_logits[mask] = float('-inf')\n",
    "                logits = sorted_logits.scatter(1, sorted_idx, sorted_logits)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "            token_id = idx_next[0].item()\n",
    "            all_generated.append(token_id)\n",
    "\n",
    "            if callback:\n",
    "                callback(token_id)\n",
    "\n",
    "            if stop_strings and decode_fn:\n",
    "                text = decode_fn(all_generated)\n",
    "                if any(s in text for s in stop_strings):\n",
    "                    break\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 8. REWARD MODEL (for RLHF)\n",
    "#========================================================\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Same transformer, but instead of predicting next token it outputs\n",
    "    a scalar score for 'how good is this response'. Used to train the\n",
    "    main model via reinforcement learning (RLHF).\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.emb_scale = math.sqrt(config.n_embd)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(i, config) for i in range(config.n_layer)\n",
    "        ])\n",
    "        self.norm_out = RMSNorm(config.n_embd)\n",
    "        self.score_head = nn.Linear(config.n_embd, 1, bias=False)\n",
    "\n",
    "    def forward(self, idx: Tensor) -> Tensor:\n",
    "        x = self.tok_emb(idx) * self.emb_scale\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, _, _ = layer(x)\n",
    "        x = self.norm_out(x)\n",
    "        return self.score_head(x[:, -1, :]).squeeze(-1)\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 9. SPECULATIVE DECODING\n",
    "#========================================================\n",
    "@torch.no_grad()\n",
    "def speculative_decode(target: UltimateClaude, draft: UltimateClaude,\n",
    "                       idx: Tensor, max_new_tokens: int,\n",
    "                       n_draft: int = 4, temperature: float = 0.8) -> Tensor:\n",
    "    \"\"\"Draft model proposes tokens fast, target model verifies in parallel.\n",
    "    Accepted tokens are 'free'; rejected ones get resampled from target.\n",
    "    Typical acceptance rate: 70-85%, giving ~3x speedup.\"\"\"\n",
    "    generated = 0\n",
    "    while generated < max_new_tokens:\n",
    "        # Draft: generate n_draft candidates autoregressively\n",
    "        draft_idx = idx.clone()\n",
    "        draft_probs_list = []\n",
    "        for _ in range(min(n_draft, max_new_tokens - generated)):\n",
    "            logits, _, _ = draft(draft_idx[:, -draft.config.block_size:])\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            dp = F.softmax(logits, dim=-1)\n",
    "            draft_probs_list.append(dp)\n",
    "            draft_idx = torch.cat([draft_idx, torch.multinomial(dp, 1)], dim=1)\n",
    "\n",
    "        n_proposed = len(draft_probs_list)\n",
    "\n",
    "        # Target: verify all draft tokens in ONE forward pass\n",
    "        verify_input = draft_idx[:, -target.config.block_size:]\n",
    "        target_logits, _, _ = target(verify_input)\n",
    "\n",
    "        # Accept/reject each draft token\n",
    "        n_accepted = 0\n",
    "        for i in range(n_proposed):\n",
    "            pos = -(n_proposed - i)\n",
    "            tp = F.softmax(target_logits[:, pos - 1, :] / temperature, dim=-1)\n",
    "            tok = draft_idx[:, idx.shape[1] + i]\n",
    "            p_t = tp[0, tok[0]].item()\n",
    "            p_d = draft_probs_list[i][0, tok[0]].item()\n",
    "\n",
    "            if torch.rand(1).item() < min(1.0, p_t / max(p_d, 1e-10)):\n",
    "                n_accepted += 1\n",
    "            else:\n",
    "                adjusted = F.relu(tp - draft_probs_list[i])\n",
    "                adjusted = adjusted / (adjusted.sum(dim=-1, keepdim=True) + 1e-10)\n",
    "                new_tok = torch.multinomial(adjusted, 1)\n",
    "                idx = torch.cat([idx, draft_idx[:, idx.shape[1]:idx.shape[1]+n_accepted], new_tok], dim=1)\n",
    "                generated += n_accepted + 1\n",
    "                break\n",
    "        else:\n",
    "            bonus = F.softmax(target_logits[:, -1, :] / temperature, dim=-1)\n",
    "            idx = torch.cat([idx, draft_idx[:, idx.shape[1]:], torch.multinomial(bonus, 1)], dim=1)\n",
    "            generated += n_proposed + 1\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 10. RLHF TRAINING PIPELINE\n",
    "#========================================================\n",
    "# RLHF turns a text-completion engine into a helpful assistant.\n",
    "# Four models work together:\n",
    "#   1. Policy  (trainable)  — the model we're improving\n",
    "#   2. Reference (frozen)   — copy of policy BEFORE RLHF, prevents drift\n",
    "#   3. Reward model (frozen) — scores \"how good\" a response is\n",
    "#   4. Value head (trainable) — estimates expected future reward at each token\n",
    "#\n",
    "# Training loop (PPO):\n",
    "#   For each prompt:\n",
    "#     → Policy generates a response\n",
    "#     → Reward model scores it\n",
    "#     → Compare policy vs reference to compute KL penalty\n",
    "#     → Compute advantages (how much better than expected)\n",
    "#     → Update policy with clipped PPO objective\n",
    "#========================================================\n",
    "\n",
    "def compute_log_probs(model: UltimateClaude, input_ids: Tensor) -> Tensor:\n",
    "    \"\"\"Compute per-token log probabilities for a sequence.\n",
    "    Returns log P(token_t | tokens_0..t-1) for each position.\"\"\"\n",
    "    logits, _, _ = model(input_ids)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    # Gather log prob of each actual next token\n",
    "    # logits[:, t, :] predicts token at position t+1\n",
    "    target_tokens = input_ids[:, 1:].unsqueeze(-1)\n",
    "    token_log_probs = log_probs[:, :-1, :].gather(-1, target_tokens).squeeze(-1)\n",
    "    return token_log_probs\n",
    "\n",
    "\n",
    "class PolicyWithValueHead(nn.Module):\n",
    "    \"\"\"Wraps UltimateClaude with a value head for PPO.\n",
    "    The value head shares the transformer body but outputs a scalar\n",
    "    estimate of 'how much reward will the rest of this sequence get'.\"\"\"\n",
    "    def __init__(self, policy: UltimateClaude):\n",
    "        super().__init__()\n",
    "        self.policy = policy\n",
    "        self.value_head = nn.Linear(policy.config.n_embd, 1, bias=False)\n",
    "        nn.init.zeros_(self.value_head.weight)\n",
    "\n",
    "    def forward(self, idx: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Returns (logits, values) — values are per-token reward estimates.\"\"\"\n",
    "        x = self.policy.tok_emb(idx) * self.policy.emb_scale\n",
    "        x = self.policy.emb_dropout(x)\n",
    "        for layer in self.policy.layers:\n",
    "            x, _, _ = layer(x)\n",
    "        x = self.policy.norm_out(x)\n",
    "        logits = self.policy.lm_head(x)\n",
    "        values = self.value_head(x).squeeze(-1)\n",
    "        return logits, values\n",
    "\n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.policy.generate(*args, **kwargs)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_preference_data(model: UltimateClaude, prompts: List[str],\n",
    "                             encode: Callable, decode: Callable) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Generate synthetic preference pairs.\n",
    "    For each prompt, generate multiple responses at different temperatures.\n",
    "    Rank by perplexity (lower = more coherent = 'chosen').\n",
    "\n",
    "    In production RLHF, this data comes from human annotators ranking\n",
    "    response pairs. Here we approximate with self-play.\"\"\"\n",
    "    model.eval()\n",
    "    preferences = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        formatted = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "        prompt_ids = encode(formatted)\n",
    "        input_tensor = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
    "\n",
    "        responses = []\n",
    "        for temp in [0.5, 1.2]:  # low temp = coherent, high temp = creative/noisy\n",
    "            output = model.generate(input_tensor.clone(), max_new_tokens=80,\n",
    "                                    temperature=temp, top_k=40, top_p=0.9)\n",
    "            response_ids = output[0, len(prompt_ids):].tolist()\n",
    "            response_text = decode(response_ids)\n",
    "            # Score by perplexity (use the model's own loss as proxy)\n",
    "            with torch.no_grad():\n",
    "                _, loss, _ = model(output, output.clone())\n",
    "            responses.append((response_text, loss.item() if loss is not None else float('inf')))\n",
    "\n",
    "        # Sort by loss: lower loss = more coherent = \"chosen\"\n",
    "        responses.sort(key=lambda x: x[1])\n",
    "        chosen = responses[0][0]\n",
    "        rejected = responses[-1][0]\n",
    "\n",
    "        if chosen != rejected:\n",
    "            preferences.append((formatted, chosen, rejected))\n",
    "            print(f\"  Prompt: {prompt[:50]}...\")\n",
    "            print(f\"    Chosen  (loss={responses[0][1]:.3f}): {chosen[:60]}...\")\n",
    "            print(f\"    Rejected(loss={responses[-1][1]:.3f}): {rejected[:60]}...\")\n",
    "\n",
    "    print(f\"\\nGenerated {len(preferences)} preference pairs\")\n",
    "    return preferences\n",
    "\n",
    "\n",
    "def train_reward_model(reward_model: RewardModel, preferences: List[Tuple[str, str, str]],\n",
    "                       encode: Callable, n_epochs: int = 3, lr: float = 1e-4):\n",
    "    \"\"\"Train the reward model on preference pairs.\n",
    "    Loss: -log(sigmoid(r_chosen - r_rejected))\n",
    "    This is the Bradley-Terry model — the probability that response A is\n",
    "    preferred over response B is sigmoid(r_A - r_B).\"\"\"\n",
    "    optimizer = torch.optim.AdamW(reward_model.parameters(), lr=lr)\n",
    "    reward_model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for prompt, chosen, rejected in preferences:\n",
    "            chosen_ids = torch.tensor([encode(prompt + chosen)], dtype=torch.long, device=device)\n",
    "            rejected_ids = torch.tensor([encode(prompt + rejected)], dtype=torch.long, device=device)\n",
    "\n",
    "            # Truncate to reasonable length\n",
    "            chosen_ids = chosen_ids[:, :512]\n",
    "            rejected_ids = rejected_ids[:, :512]\n",
    "\n",
    "            r_chosen = reward_model(chosen_ids)\n",
    "            r_rejected = reward_model(rejected_ids)\n",
    "\n",
    "            # Bradley-Terry loss\n",
    "            loss = -F.logsigmoid(r_chosen - r_rejected).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(reward_model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if r_chosen.item() > r_rejected.item():\n",
    "                correct += 1\n",
    "\n",
    "        accuracy = correct / max(len(preferences), 1) * 100\n",
    "        avg_loss = total_loss / max(len(preferences), 1)\n",
    "        print(f\"  Reward Model Epoch {epoch+1}/{n_epochs} — Loss: {avg_loss:.4f}, Accuracy: {accuracy:.1f}%\")\n",
    "\n",
    "    reward_model.eval()\n",
    "\n",
    "\n",
    "def rlhf_ppo_train(policy_model: UltimateClaude, reward_model: RewardModel,\n",
    "                    prompts: List[str], encode: Callable, decode: Callable,\n",
    "                    n_epochs: int = 2):\n",
    "    \"\"\"Proximal Policy Optimization (PPO) for RLHF.\n",
    "\n",
    "    The heart of RLHF: adjust the policy so it generates responses\n",
    "    that score higher on the reward model, while staying close to\n",
    "    the original reference policy (via KL penalty).\"\"\"\n",
    "\n",
    "    import copy\n",
    "\n",
    "    # --- Setup the four models ---\n",
    "    # 1. Policy with value head (trainable)\n",
    "    policy = PolicyWithValueHead(policy_model).to(device)\n",
    "    policy.train()\n",
    "\n",
    "    # 2. Reference model (frozen copy — the anchor)\n",
    "    ref_model = copy.deepcopy(policy_model).to(device)\n",
    "    ref_model.eval()\n",
    "    for p in ref_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 3. Reward model is already trained and frozen\n",
    "    reward_model.eval()\n",
    "    for p in reward_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    optimizer = torch.optim.AdamW(policy.parameters(), lr=RLHF_LR)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PPO TRAINING\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Epochs: {n_epochs} | Prompts: {len(prompts)}\")\n",
    "    print(f\"  KL coeff: {RLHF_KL_COEFF} | Clip range: {RLHF_CLIP_RANGE}\")\n",
    "    print(f\"  Reference model frozen. Policy training begins.\\n\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_rewards = []\n",
    "        epoch_policy_loss = []\n",
    "        epoch_value_loss = []\n",
    "        epoch_kl = []\n",
    "\n",
    "        for prompt_text in prompts:\n",
    "            # --- ROLLOUT: Generate a response from current policy ---\n",
    "            formatted = f\"### Instruction:\\n{prompt_text}\\n\\n### Response:\\n\"\n",
    "            prompt_ids = encode(formatted)\n",
    "            prompt_len = len(prompt_ids)\n",
    "            input_tensor = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = policy.generate(input_tensor.clone(), max_new_tokens=80,\n",
    "                                         temperature=0.8, top_k=40)\n",
    "            full_ids = output  # [1, prompt_len + response_len]\n",
    "            if full_ids.shape[1] <= prompt_len + 1:\n",
    "                continue  # Skip if no response generated\n",
    "\n",
    "            # --- SCORING ---\n",
    "            # Reward from the reward model\n",
    "            with torch.no_grad():\n",
    "                reward = reward_model(full_ids).item()\n",
    "            epoch_rewards.append(reward)\n",
    "\n",
    "            # Log probs from policy and reference\n",
    "            with torch.no_grad():\n",
    "                old_log_probs = compute_log_probs(policy.policy, full_ids)\n",
    "                ref_log_probs = compute_log_probs(ref_model, full_ids)\n",
    "\n",
    "            # Value estimates from the value head\n",
    "            with torch.no_grad():\n",
    "                _, old_values = policy(full_ids)\n",
    "                old_values = old_values[:, :-1]  # align with log_probs\n",
    "\n",
    "            # --- COMPUTE ADVANTAGES ---\n",
    "            # Only for response tokens (not the prompt)\n",
    "            resp_start = prompt_len - 1  # -1 because log_probs is shifted\n",
    "            resp_old_log = old_log_probs[:, resp_start:]\n",
    "            resp_ref_log = ref_log_probs[:, resp_start:]\n",
    "            resp_values = old_values[:, resp_start:]\n",
    "\n",
    "            # KL penalty per token\n",
    "            kl = resp_old_log - resp_ref_log\n",
    "            epoch_kl.append(kl.mean().item())\n",
    "\n",
    "            # Per-token rewards: KL penalty everywhere, actual reward at last token\n",
    "            per_token_reward = -RLHF_KL_COEFF * kl\n",
    "            per_token_reward[:, -1] += reward\n",
    "\n",
    "            # Advantages = rewards - value baseline\n",
    "            advantages = per_token_reward - resp_values.detach()\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            # Return targets for value head\n",
    "            returns = per_token_reward.detach()\n",
    "\n",
    "            # --- PPO UPDATE ---\n",
    "            # Recompute log probs under CURRENT policy (may have changed)\n",
    "            new_logits, new_values = policy(full_ids)\n",
    "            new_log_probs = F.log_softmax(new_logits, dim=-1)\n",
    "            new_token_log = new_log_probs[:, :-1, :].gather(\n",
    "                -1, full_ids[:, 1:].unsqueeze(-1)\n",
    "            ).squeeze(-1)\n",
    "            new_resp_log = new_token_log[:, resp_start:]\n",
    "            new_resp_values = new_values[:, :-1][:, resp_start:]\n",
    "\n",
    "            # PPO ratio\n",
    "            ratio = torch.exp(new_resp_log - resp_old_log.detach())\n",
    "            clipped_ratio = torch.clamp(ratio, 1.0 - RLHF_CLIP_RANGE, 1.0 + RLHF_CLIP_RANGE)\n",
    "\n",
    "            # Policy loss (maximize advantage, clipped)\n",
    "            policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "\n",
    "            # Value loss (predict returns accurately)\n",
    "            value_loss = F.mse_loss(new_resp_values, returns)\n",
    "\n",
    "            # Total loss\n",
    "            loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_policy_loss.append(policy_loss.item())\n",
    "            epoch_value_loss.append(value_loss.item())\n",
    "\n",
    "        # Epoch summary\n",
    "        avg_reward = sum(epoch_rewards) / max(len(epoch_rewards), 1)\n",
    "        avg_ploss = sum(epoch_policy_loss) / max(len(epoch_policy_loss), 1)\n",
    "        avg_vloss = sum(epoch_value_loss) / max(len(epoch_value_loss), 1)\n",
    "        avg_kl = sum(epoch_kl) / max(len(epoch_kl), 1)\n",
    "        print(f\"  PPO Epoch {epoch+1}/{n_epochs} — \"\n",
    "              f\"Reward: {avg_reward:.4f} | Policy Loss: {avg_ploss:.4f} | \"\n",
    "              f\"Value Loss: {avg_vloss:.4f} | KL: {avg_kl:.4f}\")\n",
    "\n",
    "    # Copy trained weights back to the original model\n",
    "    policy_model.load_state_dict(policy.policy.state_dict())\n",
    "    print(f\"\\nRLHF complete. Policy updated.\")\n",
    "    return policy_model\n",
    "\n",
    "\n",
    "def run_rlhf_pipeline(model: UltimateClaude, encode: Callable, decode: Callable,\n",
    "                      config: ModelConfig):\n",
    "    \"\"\"Full RLHF pipeline: generate preferences → train reward → PPO.\n",
    "\n",
    "    In production:\n",
    "      - Preference data comes from thousands of human annotators\n",
    "      - Reward model is trained on millions of comparisons\n",
    "      - PPO runs for thousands of steps across many GPUs\n",
    "      - Constitutional AI adds a self-critique loop\n",
    "\n",
    "    Here we demonstrate the full pipeline at nano scale.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"RLHF PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Step 1: Generate synthetic preference data\n",
    "    print(\"\\n--- Step 1: Generating preference data ---\")\n",
    "    preferences = generate_preference_data(model, RLHF_PROMPTS, encode, decode)\n",
    "\n",
    "    if len(preferences) < 2:\n",
    "        print(\"Not enough preference pairs generated. Skipping RLHF.\")\n",
    "        return model\n",
    "\n",
    "    # Step 2: Train reward model\n",
    "    print(\"\\n--- Step 2: Training reward model ---\")\n",
    "    reward_config = ModelConfig(\n",
    "        vocab_size=config.vocab_size,\n",
    "        n_layer=max(2, config.n_layer // 2),  # Smaller than policy\n",
    "        n_head=config.n_head,\n",
    "        n_kv_head=config.n_kv_head,\n",
    "        n_embd=config.n_embd,\n",
    "        block_size=config.block_size,\n",
    "        n_expert=1,  # Dense (no MoE for reward model)\n",
    "    )\n",
    "    reward_model = RewardModel(reward_config).to(device)\n",
    "    print(f\"  Reward model: {sum(p.numel() for p in reward_model.parameters())/1e6:.2f}M params\")\n",
    "    train_reward_model(reward_model, preferences, encode, n_epochs=3)\n",
    "\n",
    "    # Step 3: PPO\n",
    "    print(\"\\n--- Step 3: PPO training ---\")\n",
    "    model = rlhf_ppo_train(model, reward_model, RLHF_PROMPTS, encode, decode,\n",
    "                           n_epochs=RLHF_EPOCHS)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 11. TRAINING UTILITIES\n",
    "#========================================================\n",
    "def get_lr(it: int, config: ModelConfig) -> float:\n",
    "    \"\"\"Cosine learning rate schedule with linear warmup.\"\"\"\n",
    "    if it < config.warmup_iters:\n",
    "        return config.learning_rate * it / max(config.warmup_iters, 1)\n",
    "    if it > config.max_iters:\n",
    "        return config.learning_rate * 0.1\n",
    "    decay_ratio = (it - config.warmup_iters) / (config.max_iters - config.warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return 0.1 * config.learning_rate + coeff * 0.9 * config.learning_rate\n",
    "\n",
    "\n",
    "def get_batch(data: Tensor, config: ModelConfig) -> Tuple[Tensor, Tensor]:\n",
    "    ix = torch.randint(len(data) - config.block_size, (config.batch_size,))\n",
    "    x = torch.stack([data[i:i+config.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model: UltimateClaude, train_data: Tensor, val_data: Tensor,\n",
    "                  config: ModelConfig) -> Dict[str, float]:\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    eval_iters = 5 if device == 'cpu' else 200\n",
    "    for split, data in [('train', train_data), ('val', val_data)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(data, config)\n",
    "            _, loss, _ = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 11. DATA LOADING\n",
    "#========================================================\n",
    "def load_training_data(local_file: str) -> Optional[str]:\n",
    "    if os.path.exists(local_file):\n",
    "        try:\n",
    "            with open(local_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                text = f.read()\n",
    "            print(f\"Loaded {len(text):,} characters from {local_file}\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading local file: {e}\")\n",
    "    try:\n",
    "        import urllib.request\n",
    "        print(f\"Downloading training data from {TRAINING_DATA_URL}...\")\n",
    "        with urllib.request.urlopen(TRAINING_DATA_URL) as response:\n",
    "            text = response.read().decode('utf-8', errors='replace')\n",
    "            print(f\"Downloaded {len(text):,} characters\")\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "    return \"To be, or not to be, that is the question.\"\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 12. MODEL SAVE / LOAD\n",
    "#========================================================\n",
    "def get_save_path(filename):\n",
    "    try:\n",
    "        import google.colab\n",
    "        save_dir = '/content'\n",
    "    except ImportError:\n",
    "        save_dir = os.getcwd()\n",
    "    if not os.path.isabs(filename):\n",
    "        return os.path.join(save_dir, filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def save_model(model: UltimateClaude, filepath: str, vocab_size: int,\n",
    "               stoi: dict, itos: dict, config: ModelConfig,\n",
    "               tokenization_mode: str = 'bpe'):\n",
    "    if not os.path.isabs(filepath):\n",
    "        filepath = get_save_path(filepath)\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab_size': vocab_size,\n",
    "        'stoi': stoi,\n",
    "        'itos': itos,\n",
    "        'tokenization_mode': tokenization_mode,\n",
    "        'config': {\n",
    "            'n_layer': config.n_layer,\n",
    "            'n_head': config.n_head,\n",
    "            'n_kv_head': config.n_kv_head,\n",
    "            'n_embd': config.n_embd,\n",
    "            'block_size': config.block_size,\n",
    "            'dropout': config.dropout,\n",
    "            'n_expert': config.n_expert,\n",
    "            'n_expert_active': config.n_expert_active,\n",
    "            'moe_every_n': config.moe_every_n,\n",
    "            'sliding_window': config.sliding_window,\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Model saved to {filepath} ({os.path.getsize(filepath)/1024/1024:.2f} MB)\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def load_model(filepath: str):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Model file '{filepath}' not found!\")\n",
    "        return None, None, None, None, None\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    saved_vocab = checkpoint['vocab_size']\n",
    "    stoi = checkpoint.get('stoi', {})\n",
    "    itos = checkpoint.get('itos', {})\n",
    "    tokenization_mode = checkpoint.get('tokenization_mode', 'bpe')\n",
    "    sc = checkpoint['config']\n",
    "    model_config = ModelConfig(\n",
    "        vocab_size=saved_vocab,\n",
    "        n_layer=sc['n_layer'], n_head=sc['n_head'],\n",
    "        n_kv_head=sc.get('n_kv_head', sc['n_head']),\n",
    "        n_embd=sc['n_embd'], block_size=sc['block_size'],\n",
    "        dropout=sc['dropout'],\n",
    "        n_expert=sc.get('n_expert', 1),\n",
    "        n_expert_active=sc.get('n_expert_active', 1),\n",
    "        moe_every_n=sc.get('moe_every_n', 2),\n",
    "        sliding_window=sc.get('sliding_window', 0),\n",
    "    )\n",
    "    model = UltimateClaude(model_config).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    print(f\"   Vocab: {saved_vocab:,} | Tokenization: {tokenization_mode} | Params: {model.get_num_params()/1e6:.2f}M\")\n",
    "    return model, stoi, itos, saved_vocab, tokenization_mode\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 13. GITHUB UTILITIES\n",
    "#========================================================\n",
    "def get_github_token():\n",
    "    token_files = [GITHUB_TOKEN_FILE, f'/content/{GITHUB_TOKEN_FILE}',\n",
    "                   os.path.join(os.getcwd(), GITHUB_TOKEN_FILE)]\n",
    "    for tf in token_files:\n",
    "        if os.path.exists(tf):\n",
    "            try:\n",
    "                with open(tf, 'r') as f:\n",
    "                    token = f.readline().strip()\n",
    "                    if token and not token.startswith('#'):\n",
    "                        return token\n",
    "            except:\n",
    "                continue\n",
    "    token = os.environ.get('GITHUB_TOKEN')\n",
    "    if token:\n",
    "        return token\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_to_github_release(filename):\n",
    "    import requests\n",
    "    from datetime import datetime\n",
    "    USERNAME, REPO_NAME = \"diidihamm\", \"Project_7\"\n",
    "    try:\n",
    "        TOKEN = get_github_token()\n",
    "        if not TOKEN:\n",
    "            print(\"No GitHub token found, skipping upload.\")\n",
    "            return False\n",
    "        filepath = filename if os.path.isabs(filename) else get_save_path(filename)\n",
    "        asset_name = os.path.basename(filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            return False\n",
    "        file_size = os.path.getsize(filepath)\n",
    "        print(f\"Uploading: {asset_name} ({file_size / (1024*1024):.2f} MB)\")\n",
    "        headers = {'Authorization': f'token {TOKEN}', 'Accept': 'application/vnd.github.v3+json'}\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        release_url = f\"https://api.github.com/repos/{USERNAME}/{REPO_NAME}/releases\"\n",
    "        release_data = {'tag_name': f\"model-{timestamp}\",\n",
    "                        'name': f\"UltimateClaude Model - {datetime.now()}\", 'draft': False}\n",
    "        resp = requests.post(release_url, headers=headers, json=release_data)\n",
    "        if resp.status_code != 201:\n",
    "            print(f\"Failed to create release: {resp.status_code}\")\n",
    "            return False\n",
    "        upload_url = resp.json()['upload_url'].replace('{?name,label}', '')\n",
    "        upload_headers = {'Authorization': f'token {TOKEN}', 'Content-Type': 'application/octet-stream'}\n",
    "        with open(filepath, 'rb') as f:\n",
    "            upload_resp = requests.post(f\"{upload_url}?name={asset_name}\",\n",
    "                                        headers=upload_headers, data=f.read())\n",
    "        if upload_resp.status_code == 201:\n",
    "            print(f\"Uploaded to GitHub Releases: {upload_resp.json()['browser_download_url']}\")\n",
    "            return True\n",
    "        print(f\"Upload failed: {upload_resp.status_code}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"GitHub upload error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 14. INTERACTIVE CHAT\n",
    "#========================================================\n",
    "def interactive_chat(model: UltimateClaude, encode: Callable, decode: Callable):\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ULTIMATE CLAUDE — INTERACTIVE CHAT (Alpaca format)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nCommands: 'quit', 'settings', 'help', 'clear', 'stats'\")\n",
    "\n",
    "    params = {'max_tokens': 300, 'temperature': 0.7, 'top_k': 40,\n",
    "              'top_p': 0.9, 'repetition_penalty': 1.2}\n",
    "    stop_strings = [\"<|endoftext|>\", \"### Instruction:\", \"### Input:\"]\n",
    "\n",
    "    print(f\"\\nGeneration: max_tokens={params['max_tokens']}, temp={params['temperature']}\")\n",
    "    print(f\"Stop strings: {stop_strings}\")\n",
    "    print(f\"Architecture: RoPE + RMSNorm + SwiGLU + GQA + MoE + Sliding Window\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"\\nYou: \").strip()\n",
    "            if not prompt:\n",
    "                continue\n",
    "\n",
    "            cmd = prompt.lower()\n",
    "            if cmd in ['quit', 'exit', 'q']:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            elif cmd == 'help':\n",
    "                print(\"\\nCommands: quit, settings, help, clear, stats\")\n",
    "                continue\n",
    "            elif cmd == 'clear':\n",
    "                os.system('cls' if os.name == 'nt' else 'clear')\n",
    "                continue\n",
    "            elif cmd == 'stats':\n",
    "                print(f\"\\nModel: {model.get_num_params()/1e6:.2f}M parameters\")\n",
    "                print(f\"Config: {model.config.n_layer} layers, {model.config.n_head} Q-heads, \"\n",
    "                      f\"{model.config.n_kv_head} KV-heads\")\n",
    "                moe_layers = sum(1 for i in range(model.config.n_layer)\n",
    "                                 if model.config.n_expert > 1 and i % model.config.moe_every_n == 0)\n",
    "                print(f\"MoE layers: {moe_layers}/{model.config.n_layer} \"\n",
    "                      f\"({model.config.n_expert} experts, top-{model.config.n_expert_active})\")\n",
    "                print(f\"Sliding window: {model.config.sliding_window}\")\n",
    "                continue\n",
    "            elif cmd == 'settings':\n",
    "                print(f\"\\n1. max_tokens: {params['max_tokens']}\")\n",
    "                print(f\"2. temperature: {params['temperature']}\")\n",
    "                print(f\"3. top_k: {params['top_k']}\")\n",
    "                print(f\"4. top_p: {params['top_p']}\")\n",
    "                print(f\"5. repetition_penalty: {params['repetition_penalty']}\")\n",
    "                choice = input(\"Change (1-5): \").strip()\n",
    "                try:\n",
    "                    if choice == '1': params['max_tokens'] = int(input(\"New value: \"))\n",
    "                    elif choice == '2': params['temperature'] = float(input(\"New value: \"))\n",
    "                    elif choice == '3': params['top_k'] = int(input(\"New value: \"))\n",
    "                    elif choice == '4': params['top_p'] = float(input(\"New value: \"))\n",
    "                    elif choice == '5': params['repetition_penalty'] = float(input(\"New value: \"))\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input.\")\n",
    "                continue\n",
    "\n",
    "            # Format prompt as Alpaca\n",
    "            formatted = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "\n",
    "            print(f\"\\nUltimateClaude: \", end='', flush=True)\n",
    "            context = torch.tensor([encode(formatted)], dtype=torch.long, device=device)\n",
    "            generated_tokens = []\n",
    "            start_time = time.time()\n",
    "\n",
    "            def stream_callback(token_id: int):\n",
    "                generated_tokens.append(token_id)\n",
    "                print(decode([token_id]), end='', flush=True)\n",
    "\n",
    "            model.generate(\n",
    "                context,\n",
    "                max_new_tokens=params['max_tokens'],\n",
    "                temperature=params['temperature'],\n",
    "                top_k=params['top_k'],\n",
    "                top_p=params['top_p'],\n",
    "                repetition_penalty=params['repetition_penalty'],\n",
    "                stop_strings=stop_strings,\n",
    "                decode_fn=decode,\n",
    "                callback=stream_callback,\n",
    "            )\n",
    "\n",
    "            gen_time = time.time() - start_time\n",
    "            tok_per_sec = len(generated_tokens) / gen_time if gen_time > 0 else 0\n",
    "            print(f\"\\n\\n[{len(generated_tokens)} tokens in {gen_time:.2f}s ({tok_per_sec:.1f} tok/s)]\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nUse 'quit' to exit.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# --- 15. MAIN\n",
    "#========================================================\n",
    "def main():\n",
    "    global config\n",
    "\n",
    "    print(f\"\\nThe current folder is: {os.getcwd()}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    model_path = get_save_path(MODEL_FILENAME)\n",
    "\n",
    "    # --- Try loading existing model ---\n",
    "    if not TRAIN_NEW_MODEL and os.path.exists(model_path):\n",
    "        print(\"Loading existing model...\")\n",
    "        model, stoi, itos, vocab_size, tokenization_mode = load_model(model_path)\n",
    "        if model is not None:\n",
    "            if tokenization_mode == 'bpe':\n",
    "                enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "                encode = lambda s: enc.encode(s, allowed_special='all')\n",
    "                decode = lambda l: enc.decode(l)\n",
    "            else:\n",
    "                encode = lambda s: [stoi.get(c, 0) for c in s]\n",
    "                decode = lambda l: ''.join([itos.get(i, '') for i in l])\n",
    "            interactive_chat(model, encode, decode)\n",
    "            return\n",
    "\n",
    "    # --- Train new model ---\n",
    "    print(\"Training new model...\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nArchitecture: RoPE + RMSNorm + SwiGLU + GQA + QK-Norm + MoE\")\n",
    "    print(f\"  Layers: {config.n_layer} | Q-heads: {config.n_head} | KV-heads: {config.n_kv_head}\")\n",
    "    print(f\"  Experts: {config.n_expert} (top-{config.n_expert_active}) | Sliding window: {config.sliding_window}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    text = load_training_data(LOCAL_FILE)\n",
    "\n",
    "    # Setup tokenizer\n",
    "    stoi, itos = {}, {}\n",
    "    if TOKENIZATION_MODE == 'bpe':\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        vocab_size = enc.n_vocab\n",
    "        config.vocab_size = vocab_size\n",
    "        encode = lambda s: enc.encode(s, allowed_special='all')\n",
    "        decode = lambda l: enc.decode(l)\n",
    "    else:\n",
    "        chars = sorted(list(set(text)))\n",
    "        vocab_size = len(chars)\n",
    "        config.vocab_size = vocab_size\n",
    "        stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        itos = {i: ch for i, ch in enumerate(chars)}\n",
    "        encode = lambda s: [stoi.get(c, 0) for c in s]\n",
    "        decode = lambda l: ''.join([itos.get(i, '') for i in l])\n",
    "\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    n = int(0.9 * len(data))\n",
    "    train_data, val_data = data[:n], data[n:]\n",
    "\n",
    "    # Shrink block_size if dataset is too small\n",
    "    min_tokens = min(len(train_data), len(val_data))\n",
    "    if min_tokens <= config.block_size:\n",
    "        config.block_size = max(8, min_tokens - 1)\n",
    "        print(f\"WARNING: Small dataset — block_size reduced to {config.block_size}\")\n",
    "\n",
    "    print(f\"Training: {len(train_data):,} tokens | Validation: {len(val_data):,} tokens\")\n",
    "\n",
    "    model = UltimateClaude(config).to(device)\n",
    "    print(f\"Model: {model.get_num_params()/1e6:.2f}M parameters\")\n",
    "\n",
    "    # Report MoE layer distribution\n",
    "    moe_layers = [i for i in range(config.n_layer)\n",
    "                  if config.n_expert > 1 and i % config.moe_every_n == 0]\n",
    "    dense_layers = [i for i in range(config.n_layer) if i not in moe_layers]\n",
    "    print(f\"  MoE layers: {moe_layers} | Dense layers: {dense_layers}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    use_amp = device == 'cuda'\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "\n",
    "    start_time = time.time()\n",
    "    pbar = tqdm(range(config.max_iters), desc=\"Training Progress\")\n",
    "\n",
    "    for iter_num in pbar:\n",
    "        if iter_num % config.eval_interval == 0:\n",
    "            losses = estimate_loss(model, train_data, val_data, config)\n",
    "            msg = f\"Step {iter_num} | Train: {losses['train']:.4f} | Val: {losses['val']:.4f}\"\n",
    "            print(f\"\\n{msg}\")\n",
    "            pbar.set_description(msg)\n",
    "\n",
    "        xb, yb = get_batch(train_data, config)\n",
    "        lr = get_lr(iter_num, config)\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg['lr'] = lr\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                _, loss, _ = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            _, loss, _ = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\nTraining complete in {elapsed:.2f} min\")\n",
    "\n",
    "    # Final eval\n",
    "    final_losses = estimate_loss(model, train_data, val_data, config)\n",
    "    print(f\"Final — Train: {final_losses['train']:.4f} | Val: {final_losses['val']:.4f}\")\n",
    "\n",
    "    # Save\n",
    "    print(\"\\nSaving model...\")\n",
    "    saved_path = save_model(model, MODEL_FILENAME, vocab_size, stoi, itos, config, TOKENIZATION_MODE)\n",
    "\n",
    "    if saved_path:\n",
    "        print(\"\\nUploading to GitHub Releases...\")\n",
    "        save_to_github_release(saved_path)\n",
    "\n",
    "    # --- RLHF phase (optional) ---\n",
    "    if RUN_RLHF:\n",
    "        model = run_rlhf_pipeline(model, encode, decode, config)\n",
    "        # Save the RLHF-aligned model\n",
    "        rlhf_path = MODEL_FILENAME.replace('.pth', '_rlhf.pth')\n",
    "        print(\"\\nSaving RLHF-aligned model...\")\n",
    "        save_model(model, rlhf_path, vocab_size, stoi, itos, config, TOKENIZATION_MODE)\n",
    "\n",
    "    interactive_chat(model, encode, decode)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nGoodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
